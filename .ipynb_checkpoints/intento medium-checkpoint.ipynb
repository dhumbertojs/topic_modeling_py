{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Esto viene de un post de medium \n",
    "#https://medium.com/@armandj.olivares/topic-modeling-on-spanish-text-f7a5e998fb90\n",
    "##El githu es este: https://github.com/ArmandDS/topic_modeling/blob/master/topic_modeling_spanish.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv(\"/content/gdrive/My Drive/text_summarization/df_news_20minutos.csv\")\n",
    "df_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "df_news.dropna(inplace=True)\n",
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = ['más', 'mas', 'unir', 'paises', 'pais', 'espa', 'no', 'os', 'a', 'compa', 'acompa', 'off', 'and', 'grecia', 'the','it', 'to',\n",
    "              'd',  'et',  'dame',  'il',  'dans', 'that',  'as',   'for',  'it',  'elections',  'would',  'this',  'with', 'york', 'obama', 'chavez', 'gadafi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(word):\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
    "  word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", word)\n",
    "  word = re.sub(r'ee.uu', 'eeuu', word)\n",
    "  word = re.sub(r'\\#\\.', '', word)\n",
    "  word = re.sub(r'\\n', '', word)\n",
    "  word = re.sub(r',', '', word)\n",
    "  word = re.sub(r'\\-', ' ', word)\n",
    "  word = re.sub(r'\\.{3}', ' ', word)\n",
    "  word = re.sub(r'a{2,}', 'a', word)\n",
    "  word = re.sub(r'é{2,}', 'é', word)\n",
    "  word = re.sub(r'i{2,}', 'i', word)\n",
    "  word = re.sub(r'ja{2,}', 'ja', word) \n",
    "  word = re.sub(r'á', 'a', word)\n",
    "  word = re.sub(r'é', 'e', word)\n",
    "  word = re.sub(r'í', 'i', word)\n",
    "  word = re.sub(r'ó', 'o', word)\n",
    "  word = re.sub(r'ú', 'u', word)  \n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  list_word_clean = []\n",
    "  for w1 in word.split(\" \"):\n",
    "    if  w1.lower() not in stopwords:\n",
    "      list_word_clean.append(w1.lower())\n",
    "\n",
    "  bigram_list = bigram[list_word_clean]\n",
    "  out_text = lemmatization(\" \".join(bigram_list))\n",
    "  return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    texts_out = [ token.text for token in nlp(texts) if token.pos_ in \n",
    "                 allowed_postags and token.text not in black_list and len(token.text)>2]\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_news.text.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bigram = gensim.models.Phrases(df_news.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram[df_news.text.to_list()[20000].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('spanish'))\n",
    "\n",
    "additional_stopwords=set(black_list)\n",
    "\n",
    "stopwords = stop.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/usr/local/lib/python3.6/dist-packages/es_core_news_md/es_core_news_md-2.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner(df_news.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['text2'] = df_news['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_news['text'] = df_news['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df_news['text'].to_list())\n",
    "dictionary.compactify()\n",
    "# Filter extremes\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
    "dictionary.compactify()\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in df_news['text'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary, random_state= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, model_type=\"lda\"):\n",
    "  for topic_idx, topic in enumerate(model.print_topics()):\n",
    "    print (\"Topic %d:\" % (topic_idx))\n",
    "    if model_type== \"hdp\":\n",
    "      print (\" \".join(re.findall( r'\\*(.[^\\*-S]+).?', topic[1])), \"\\n\")\n",
    "    else:\n",
    "      print (\" \".join(re.findall( r'\\\"(.[^\"]+).?', topic[1])), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(hdpmodel, model_type=\"hdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph(dictionary, corpus, texts, limit, model):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        if model == 'lsi':\n",
    "          lm = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        else:\n",
    "          lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsimodel)  # Showing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lmlist_lsi, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['text'].to_list(), limit=21, model= \"lsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lmlist_lsi[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['text'].to_list(), limit=21, model= \"lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = lmlist[11]\n",
    "lsimodel = lmlist_lsi[2]\n",
    "\n",
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=df_news['text'].to_list(), dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=df_news['text'].to_list(), dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=df_news['text'].to_list(), dictionary=dictionary, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "coherences = [lsi_coherence, hdp_coherence, lda_coherence]\n",
    "n = len(coherences)\n",
    "x = ['lsi_coherence','hdp_coherence', 'lda_coherence']\n",
    "sns.barplot(x, coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_16 =lmlist[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(ldamodel_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=0, corpus=corpus, texts=0):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()-n\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel, corpus=corpus, texts=df_news['text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dicc = {0:'trials', 1:'nature', 2:'violence', 3: 'protest', 4:'life_family', 5:'sea_disasters', 6:'elections', \n",
    "              7:'terrorism', 8:'economic_crisis', 9: 'seism', 10:'peacy_treaty', 11:'airport_security'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].map(label_dicc)\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['labels'] = df_dominant_topic['Dominant_Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[['text2', 'labels']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[ df_news['labels'] == 'family'].head().text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_dominant_topic['Dominant_Topic'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
